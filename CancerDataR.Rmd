---
title: "CancerDataR"
author: "Minhaz Khan, Truman Zheng, Navin Chandradat, Vincent La, Bobak Ahmar"
date: "11/13/2018"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

# Tasks:
Summary of data, Introduction - Truman

Organizing presentation - everyone

Presenting - Truman, Minhaz, Vincent

Anaylizing data/Coding - Truman, Minhaz, Vincent

Putting everything together/cConclusions/Report - Navin, Bobak


# introduction
Breast cancer is a malignant cell growth in the breast. If it is left untreated the cancer can spread to other parts of the human body and it can be very deadly. 
There are generally two type of tumors non-cancerous and cancerous and the difference between the two is important, Benign tumor is non-cancerous and not dangerous on its own, but a malignant tumor, means the mass is cancerous.

Our goal for this project is to predict whether the cancer is benign or malignant and to determine what actually contribute to the classification of the two types.

We are given the following:
Attribute Information: 1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)

a) radius (mean of distances from center to points on the perimeter) 
b) texture (standard deviation of gray-scale values) 
c) perimeter 
d) area 
e) smoothness (local variation in radius lengths) 
f)compactness (perimeter^2 / area - 1.0) 
g) concavity (severity of concave portions of the contour) 
h) concave points (number of concave portions of the contour) 
i) symmetry 
j) fractal dimension("coastline approximation" - 1)

The mean, standard error and "worst" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features
Class distribution: 357 benign, 212 malignant

## summary of the data
We first take a small look at the data set and loading library/files we will need.
```{r, results='hide',message=FALSE}
# all the library and files we'll be using
library(tidyverse)
library(gridExtra)
library(ICSNP)
library(MASS)
library(klaR)
source("Box_M.R")
```

```{r}
# preview of the data
cancer = read.csv("Project3-Data.csv")
head(cancer[1:5])
```

```{r}
# number of variables we have
num_var = ncol(cancer) - 1
num_var
# number of observation we have
num_obs = nrow(cancer)
num_obs
```

```{r}
# the number of each type of tumor
table(cancer$diagnosis)
```

## some visuals of the data
## Standard errors vs worst cases
```{r, collapse=TRUE}
# filter out the first 2 columns
filcancer1=cancer[-1:-2]

x1<-ggplot(filcancer1, aes(x=radius_worst, y = radius_se))+geom_point()
x2<-ggplot(filcancer1, aes(x=perimeter_worst, y = perimeter_se))+geom_point()
x3<-ggplot(filcancer1, aes(x=area_worst, y = area_se))+geom_point()
#If we look at the first plot of texture's worst against standard errors, we can see
#non constant variance due to the cone shape of the data.
x4<-ggplot(filcancer1, aes(x=texture_worst, y = texture_se))+geom_point()
#Again we have a cone shape in the plot of smoothness worst versus standard error.
x5<-ggplot(filcancer1, aes(x=smoothness_worst, y = smoothness_se))+geom_point()
x6<-ggplot(filcancer1, aes(x=compactness_worst, y = compactness_se))+geom_point()
x7<-ggplot(filcancer1, aes(x=concavity_worst, y = concavity_se))+geom_point()
x8<-ggplot(filcancer1, aes(x=symmetry_worst, y = symmetry_se))+geom_point()
x9<-ggplot(filcancer1, aes(x=concave_points_worst, y = concave_points_se))+geom_point()
x10<-ggplot(filcancer1, aes(x=fractal_dimension_worst, y = fractal_dimension_se))+geom_point()
#Looking at the data, most points are near the the origin. However, the further stages
#in cancer seem to have higher standard errors. Also, most of the properties seem to
#follow a slightly curved distribution.
grid.arrange(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,ncol=3)
```
If we look at all the graphs for each category for measuring breast cancer, we can see that most data points cluster quite near to the origin, but not exactly on it. There are no data points at the origin since this would mean that the person has no reason to suspect that they have breast cancer.
If we now look at the data points that are close to the center but not exactly, these points might represent either benign tumours or early stage malignant tumours, since these breasts don't really deviate that much from the average. If we look at the points further out, these points are likely middle stage malignant tumours, since they deviate a sizeable amount. 
Now, looking at the outliers, we have reason to suspect that these are late stage malignant tumours too, given the deformity of the breasts.

## Boxplot of the SEs
```{r, collapse=TRUE}
new = cancer[-1]
new2 = new[,-c(2:11)]
new3 = new2[,-c(12:21)]
x1 = ggplot(new3, aes(x=diagnosis, y=radius_se))+geom_boxplot()
x2 = ggplot(new3, aes(x=diagnosis, y=texture_se))+geom_boxplot()
x3 = ggplot(new3, aes(x=diagnosis, y=perimeter_se))+geom_boxplot()
x4 = ggplot(new3, aes(x=diagnosis, y=area_se))+geom_boxplot()
x5 = ggplot(new3, aes(x=diagnosis, y=smoothness_se))+geom_boxplot()
x6 = ggplot(new3, aes(x=diagnosis, y=compactness_se))+geom_boxplot()
x7 = ggplot(new3, aes(x=diagnosis, y=concavity_se))+geom_boxplot()
x8 = ggplot(new3, aes(x=diagnosis, y=concave_points_se))+geom_boxplot()
x9 = ggplot(new3, aes(x=diagnosis, y=symmetry_se))+geom_boxplot()
x10 = ggplot(new3, aes(x=diagnosis, y=fractal_dimension_se))+geom_boxplot()
grid.arrange(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,ncol=4)
```
Each boxplot shows symmetry regarding the SE's, could potentially say its normally distributed since we have large sample size to back up the outliers.

# methods
## Chi Square plot
before we start any analysis we want to verify the normallity of the data.
```{r}
source("ChisqPlot.R")
can = cancer[,2:12]

# setting the independent variables into a matrix
can.matrix = as.matrix(can[,2:11])
chisqplot(can.matrix)
```
becuase our data have a very large sample, I would say the normallity assumption here is fine.

## fitting a generalized linear model
we first starts of by fitting a generalized linear model to assess the significance of each of the variables. 
here we are only dealing with the "mean" variables as we believe that the other two category "standard error" and "worst" will not give us much information regarding the type of cancer.
```{r}
# taking the mean values
can = cancer[2:12]

# changing diagnosis from chr to factor so is easy for model fitting
can$diagnosis = as.factor(can$diagnosis)

# fitting the generalized linear model
glm.fit = glm(diagnosis ~ ., data=can, family=binomial)

summary(glm.fit)
```
Theres various information in the summary but look at the coefficients, we have estimate, SE, z-score, and p-value, the p-value that is less than 0.05 indicates significance, that is those variable has an impact on either cancer being M or B.
Example: for a unit increase in texture mean the odd of cancer being M (vs B) increases by exp(0.38473).

Now that we know which of the variables are actually significant, we will fit the model again with only those significant variables.
```{r}
# new model after removing insignificant variables
glm.fit2 = glm(diagnosis ~ ., data=can[,c(1,3,5,6,9)], family=binomial)

summary(glm.fit2)
```
Taking a look at the different AIC values of the two model, we see that the AIC values for the reduced model are smaller than the full model, this tells us that not only are some of the variable are insignificant but it also will effect the accuracy of our result.

With the following we can get a rough probability of type of cancer with given values.
```{r}
# a function to get the probability of cancer being type M
prob = function(x1,x2,x3,x4){
  x = exp(-23.677816 + 0.362687*x1 + 0.010342*x2 + 59.471304*x3 + 76.571210*x4)
  pix = x/(1+x)
  return(pix)
}
```

## Discriminant Analysis
Now we shall take a look at another method, here we use discriminant analysis, Discriminant analysis is a technique that is used to analyze the research data when the criterion or the dependent variable is categorical and the predictor or the independent variable is interval in nature (which is what we have here).

First we have to compute a two-sample Hotelling T-Squared test and compute Bartlett's test for homogeneous covariance matrices. with this we can determine whether or not to use Linear DA or quadratic DA as one requires equal covariance and the other one does not (LDA require equal covariance).
```{r}
# again we are only working with the means
can = cancer[,2:12]

# setting the independent variables into a matrix
can.matrix = as.matrix(can[,2:11])

fit=manova(can.matrix ~ can$diagnosis)
summary(fit, test="Hotelling-Lawley")

# create separate data sets for Benign and Malignant tumors.
cancer1 <- can[can[,1]=="M",2:11]
cancer2 <- can[can[,1]=="B",2:11]

HotellingsT2(cancer1,cancer2)

n1 = dim(cancer1)[1]
n2 = dim(cancer2)[1]

Box_M(can.matrix, n=c(n1, n2))
```
Here we see that we do not have equal covariance and so we'll be using QDA instead of LDA for better performance/accuracy.


## discriminant analysis with all 10 variables
We first start with all 10 variables just so we can have a comparison later with the reduced model.
```{r}

# spliting the data into 2 set, training and testing
training_sample <- sample(c(TRUE, FALSE), nrow(can), replace = T, prob = c(0.6,0.4))
cantrain <- can[training_sample, ]
cantest <- can[!training_sample, ]

# the model
cancer.qda <- qda(diagnosis ~ ., data=cantrain)
cancer.qda

#Confusion test
set.seed(1)
confusionTest <- table(cantest$diagnosis, predict(cancer.qda, newdata=cantest)$class)
confusionTest

n <- sum(confusionTest)
aer <- (n - sum(diag(confusionTest))) / n
aer
```

## Discriminant analysis with the significant variables
We now do the same thing but with the reduced model.
```{r}
# splitting data into 2sets, training and testing
can2 = can[,c(1,3,5,6,9)]
training_sample2 <- sample(c(TRUE, FALSE), nrow(can2), replace = T, prob = c(0.6,0.4))
cantrain2 <- can2[training_sample2, ]
cantest2 <- can2[!training_sample2, ]

# the model
cancer.qda2 <- qda(diagnosis ~ ., data=cantrain2, CV=FALSE)
cancer.qda2

# testing the accuracy of our model
set.seed(1)
qda.test <- predict(cancer.qda2,cantest2)
cantest2$qda <- qda.test$class
confusionTest <-table(cantest2$qda,cantest2$diagnosis)
confusionTest

n <- sum(confusionTest)
aer <- (n - sum(diag(confusionTest))) / n
aer
```
Taking a look at the errors of the two model, full vs reduced we see that the difference between the 2 is negligible (the AER for the two are very close).
This also however tells us that with only 4 variables, texture mean, area mean, smoothness mean and concave points mean we can accurately predict 90% of the class of observation which is very good.

```{r}
# here are just some more visuals of the data
partimat(diagnosis ~ ., data=can2, method="qda")
```
From this we can also confirm that the accuracy of our model is indeed good.

# conclusion
From the two analysis we done, from discriminant analysis to simply fitting a generalized linear model we can clearly conclude that the dependent variable cancer type or diagnosis hugely depend on simply four variables, that is, it's mainly depend on texture mean, area mean, smoothness mean and concave points mean and from these four variables we can determine the odds patient's cancer type and so from that can determine whether treatment are neccessary.

# reference
Fayed, L., & Paul, D. (n.d.). Differences Between a Malignant and Benign Tumor. Retrieved from https://www.verywellhealth.com/what-does-malignant-and-benign-mean-514240

Sign In. (n.d.). Retrieved from https://rpubs.com/Nolan/298913
