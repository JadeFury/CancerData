---
title: "CancerData"
author: "Minhaz Khan, Truman Zheng, Navin Chandradat, Vincent La, Bobak Ahmar"
date: "11/13/2018"
output: pdf_document
---

## Group number 7
## Members:
Minhaz Khan,
Bobak Ahmar,
Vincent La,
Navin Chandradat,
Truman Zheng

## Tasks:
data cleaning, summary of data, introduction - Truman

Organizing presentation/presenting - Truman, Minhaz, Vincent

Anaylizing data/performing various test - Everyone 
(idea: each of us analize different variables)

Putting everything together/conclusions - Navin, Bobak


## Introduction
Breast cancer is a malignant cell growth in the breast. If it is left untreated the cancer can spread to other parts of the human body and it can be very deadly. There are generally two type of tumors non-cancerous and cancerous and the difference between the two is important, Benign tumor is non-cancerous and not dangerous on its own, but a malignant tumor, means the mass is cancerous.


## summary of the data
```{r}
library(tidyverse)
```

```{r}
# preview of the data
cancer = read.csv("Project3-Data.csv")
head(cancer)
```

```{r}
# number of variables we have
num_var = ncol(cancer) - 1
num_var
# number of observation we have
num_obs = nrow(cancer)
num_obs
```

```{r}
# the number of each type of tumor
table(cancer$diagnosis)
```

## PCA
Attempt to perform PCA on the data. Reasoning behind attempting the procedure is
as follows - in PCA we create new variables which are linear combinations of the old ones. These new variables should be uncorrelated, so we don't have any redundant information. We'll choose only the first m variables, in a way that accounts for as much variation as possible. In our data, we showed that there are a lot of variables that are highly correlated, so we will get rid of them. The data is in different scales (area mean and texture_worst are very far out) so we need to do the analysis on the correlation matrix.
```{r}
pca1 = prcomp(filcancer, scale=TRUE)
summary(pca1)

# pr_var = ( pca1$sdev )^2 
# prop_varex = pr_var / sum( pr_var )
# plot( prop_varex, xlab = "Principal Component", ylab = "Proportion of Variance Explained", type = "b" )
# plot( cumsum( prop_varex ), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", type = "b" )

plot(pca1,type='l')
screeplot(pca1,type = 'l')
# From the scree plot, the elbow of the plot is the third PC but according to the correlation with the variables and its proportion of variance, it's insignificant, so we work with first 2 PCAs
pca2 = pca1$x[,1:3]

plot(pca2, pch=10, col="red")
cor(filcancer, pca2)
```


# Getting the variance and standard errors:
```{r}
library(tidyverse)
library(gridExtra)
filcancer1=cancer[-1:-2]
standard_errors = filcancer1[,11:20]
worst_cases = filcancer1[,21:30]
worst_data = data.frame(worst_cases)
se_data = data.frame(standard_errors)
df_radius = data.frame(cbind(rad_worst=worst_data$radius_worst,rad_se=se_data$radius_se))
df_perimeter = data.frame(cbind(peri_worst=worst_data$perimeter_worst,peri_se=se_data$radius_se))
df_area = data.frame(cbind(area_worst=worst_data$area_worst,area_se=se_data$area_se))
df_texture = data.frame(cbind(text_worst=worst_data$texture_worst,text_se=se_data$texture_se))
df_smoothness = data.frame(cbind(smooth_worst=worst_data$smoothness_worst,smooth_se=se_data$smoothness_se))
df_compact = data.frame(cbind(compact_worst=worst_data$compactness_worst,compact_se=se_data$compactness_se))
df_concavity = data.frame(cbind(concavity_worst=worst_data$concavity_worst,concavity_se=se_data$concavity_se))
df_concave_points = data.frame(cbind(concave_worst=worst_data$concave_points_worst,concave_se=se_data$concave_points_se))
df_symmetry = data.frame(cbind(symm_worst=worst_data$symmetry_worst,symm_se=se_data$symmetry_se))
df_dimension = data.frame(cbind(dim_worst=worst_data$fractal_dimension_worst,dim_se = se_data$fractal_dimension_se))


x1<-ggplot(df_radius, aes(x=rad_worst, y = rad_se))+geom_point()
x2<-ggplot(df_perimeter, aes(x=peri_worst, y = peri_se))+geom_point()
x3<-ggplot(df_area, aes(x=area_worst, y = area_se))+geom_point()
#If we look at the first plot of texture's worst against standard errors, we can see non constant variance due to the cone shape of the data.
x4<-ggplot(df_texture, aes(x=text_worst, y = text_se))+geom_point()
#Again we have a cone shape in the plot of smoothness worst versus standard error.
x5<-ggplot(df_smoothness, aes(x=smooth_worst, y = smooth_se))+geom_point()
x6<-ggplot(df_compact, aes(x=compact_worst, y = compact_se))+geom_point()
x7<-ggplot(df_concavity, aes(x=concavity_worst, y = concavity_se))+geom_point()
x8<-ggplot(df_symmetry, aes(x=symm_worst, y = symm_se))+geom_point()
x9<-ggplot(df_concave_points, aes(x=concave_worst, y = concave_se))+geom_point()
x10<-ggplot(df_dimension, aes(x=dim_worst, y = dim_se))+geom_point()
grid.arrange(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,ncol=3)
```

## fitting a generalized linear model
with this we can determine which variable are actually important to the result and which have next to no impact on the result
```{r}
can = cancer[2:12]

# changing diagnosis from chr to factor
can$diagnosis = as.factor(can$diagnosis)

# fitting the generalized linear model
glm.fit = glm(diagnosis ~ texture_mean + area_mean + smoothness_mean + compactness_mean + concavity_mean + symmetry_mean + fractal_dimension_mean + radius_mean + perimeter_mean + concave_points_mean, data=can, family=binomial)

summary(glm.fit)
```
theres various information in the summary but look at the coefficients, we have estimate, SE, z-score, and p-value, the p-value that is less than 0.05 indicates significance, that is those variable has an impact on either cancer being M or B 
example: for a unit increase in texture mean the log odd of cancer being M (vs B) increases by 0.38473

```{r}
# new model after removing insignificant variables
glm.fit2 = glm(diagnosis ~ texture_mean + area_mean + smoothness_mean + concave_points_mean, data=can, family=binomial)

summary(glm.fit2)
```

```{r}
# a function to get the probability of cancer being type M
prob = function(x1,x2,x3,x4){
  x = exp(-23.677816 + 0.362687*x1 + 0.010342*x2 + 59.471304*x3 + 76.571210*x4)
  pix = x/(1+x)
  return(pix)
}
```

======================================================================================


## Discriminant Analysis
First we have to compute a two-sample Hotelling T-Squared test and compute Bartlett's test for homogeneous covariance matrices.
```{r}
# here we specfically choose only the means of the measure as the SE and Worst will not give us very much information
can = cancer[,2:12]

# setting the independent variables into a matrix
can.matrix = as.matrix(can[,2:11])

fit=manova(can.matrix ~ can$diagnosis)
summary(fit, test="Hotelling-Lawley")

library(ICSNP)
# create separate data sets for Benign and Malignant tumors.
cancer1 <- can[can[,1]=="M",2:11]
cancer2 <- can[can[,1]=="B",2:11]

HotellingsT2(cancer1,cancer2)

n1 = dim(cancer1)[1]
n2 = dim(cancer2)[1]
source("Box_M.R")
Box_M(can.matrix, n=c(n1, n2))
```
here we see that we do not have equal covariance and so we'll be using QDA instead of LDA for better performance/accuracy

# discriminant analysis with all 10 variable
```{r}
library(MASS)
# spliting the data into 2 set, training and testing
training_sample <- sample(c(TRUE, FALSE), nrow(can), replace = T, prob = c(0.6,0.4))
cantrain <- can[training_sample, ]
cantest <- can[!training_sample, ]

# the model
cancer.qda <- qda(diagnosis ~ radius_mean+texture_mean+perimeter_mean+area_mean+smoothness_mean+compactness_mean+concavity_mean+concave_points_mean+symmetry_mean+fractal_dimension_mean, data=cantrain, CV=FALSE)
cancer.qda
```
```{r}
table(cantrain$diagnosis, predict(cancer.qda)$class)
```

# Discriminant analysis with the significant variables
```{r}
library(MASS)
# splitting data into 2sets, training and testing
can2 = can[,c(1,3,5,6,9)]
training_sample2 <- sample(c(TRUE, FALSE), nrow(can2), replace = T, prob = c(0.6,0.4))
cantrain2 <- can2[training_sample2, ]
cantest2 <- can2[!training_sample2, ]

# the model
cancer.qda2 <- qda(diagnosis ~ ., data=cantrain2, CV=FALSE)
cancer.qda2
```
```{r}
# to get some visual of the data
partimat(diagnosis ~ ., data=cantrain2, method="qda")

# QDA prediction
table(cantrain2$diagnosis, predict(cancer.qda2)$class)
```
comparing the two different DA we did on the full data and reduced data, the two are very close to each other and so we'll be working with the reduced model
Also we see that this model fits the training set very well

```{r}
# testing the accuracy of our model
qda.test <- predict(cancer.qda2,cantest2)
cantest2$qda <- qda.test$class
table(cantest2$qda,cantest2$diagnosis)
```
here we see that the model correctly predicted the class of observation 90% (211/234) which is really good
and so to determine cancer type we can look at the following four variables: texture mean, area mean, smoothness mean and concave points mean.


# Discriminant Analysis with the significant variables Continued
```{r}
p=4
g=2

cancer.lda2$scaling
crossprod(cancer.lda2$scaling, Sp) %*% cancer.lda2$scaling

(canmu.d = cancer.lda2$means)
(canmu = colMeans(canmu.d))
can2[,1:5]
#Attempt at getting the discriminant scores
(dscores <- scale(can2[,2:5], center=canmu, scale=F) %*% cancer.lda2$scaling)
predict(cancer.lda2)$x
(canid = as.integer(predict(cancer.lda2)$class))

plot(dscores, xlab="LD1",ylab = "Scores", pch=canid, col=canid,
main="Discriminant Scores", xlim=c(1, 569), ylim=c(-3, 6))
legend("top",lev,pch=1:2,col=1:2,bty="n", legend = c("M","B"))
```
#Plotting the Discriminant Boundary
```{r}
library(klaR)
cantype <- factor(rep(c("M","B"), each=200))
partimat(x=dscores, grouping=cantype, method="lda")

```
#Plotting the Decision boundary for all the variables
```{r}
library(klaR)
cantype <- factor(rep(c("M","B"), each=200))
partimat(x=can2[,2:5], grouping=cantype, method="lda")
```
#Getting the Confusion matrix and the error rate
```{r}
# make confusion matrix (and APER)
confusion <- table(can2$diagnosis, predict(cancer.lda2)$class)

confusion

n <- sum(confusion)
aper <- (n - sum(diag(confusion))) / n
aper

#Expected Error Rate
cancerCV <- lda(diagnosis ~ ., data=can2, CV=TRUE)
confusionCV <- table(can2$diagnosis, cancerCV$class)
confusionCV
eaer <- (n - sum(diag(confusionCV))) / n
eaer
```


# boxplot of all the SE
```{r}
new = cancer[-1]
new2 = new[,-c(2:11)]
new3 = new2[,-c(12:21)]
x1 = ggplot(new3, aes(x=diagnosis, y=radius_se))+geom_boxplot()
x2 = ggplot(new3, aes(x=diagnosis, y=texture_se))+geom_boxplot()
x3 = ggplot(new3, aes(x=diagnosis, y=perimeter_se))+geom_boxplot()
x4 = ggplot(new3, aes(x=diagnosis, y=area_se))+geom_boxplot()
x5 = ggplot(new3, aes(x=diagnosis, y=smoothness_se))+geom_boxplot()
x6 = ggplot(new3, aes(x=diagnosis, y=compactness_se))+geom_boxplot()
x7 = ggplot(new3, aes(x=diagnosis, y=concavity_se))+geom_boxplot()
x8 = ggplot(new3, aes(x=diagnosis, y=concave_points_se))+geom_boxplot()
x9 = ggplot(new3, aes(x=diagnosis, y=symmetry_se))+geom_boxplot()
x10 = ggplot(new3, aes(x=diagnosis, y=fractal_dimension_se))+geom_boxplot()
grid.arrange(x1,x2,x3,x4,x5,x6,x7,x8,x9,x10,ncol=4)
```

